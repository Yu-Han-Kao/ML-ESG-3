{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4861,"status":"ok","timestamp":1713975312057,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"},"user_tz":-480},"id":"h0o4k90AK5jX","outputId":"5fc73c37-d194-4d88-962d-c0e054cdadaf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# 連結Google drive檔案\n","from google.colab import drive\n","import pandas as pd\n","drive.mount('/content/drive', force_remount=True)\n","\n","file_path =(\"/content/EN_generated.xlsx\")"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9342,"status":"ok","timestamp":1713975321394,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"},"user_tz":-480},"id":"IIPRSRXQLmu_","outputId":"3dbe796c-2398-469a-9027-97cf2b32bfe3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"]}],"source":["!pip install openpyxl"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":683,"status":"ok","timestamp":1713975322073,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"},"user_tz":-480},"id":"mD_NM1eYRVNJ","outputId":"f53526c3-45d5-43b8-82ae-91671325739f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total News Content: 7499\n","Impact Duration Distribution:\n","Impact_Duration\n","0    2497\n","1    2498\n","2    2504\n","Name: count, dtype: int64\n"]}],"source":["df = pd.read_excel(file_path)\n","\n","# Calculate the total number of news content\n","total_news_content = len(df)\n","\n","# Calculate unique news titles\n","\n","\n","# Impact Duration Distribution based on the exact values of 0, 1, and 2\n","impact_duration_distribution = df['Impact_Duration'].value_counts().sort_index()\n","\n","# Impact Level Distribution\n","\n","\n","# Output the calculated values\n","print(f\"Total News Content: {total_news_content}\")\n","\n","print(\"Impact Duration Distribution:\")\n","print(impact_duration_distribution)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20329,"status":"ok","timestamp":1713975342399,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"},"user_tz":-480},"id":"vxKvUk0DRVPf","outputId":"fe2983c8-a74d-4424-d316-d86ff25f50ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers\u003c0.20,\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors\u003e=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec\u003e=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.19.3-\u003etransformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.19.3-\u003etransformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.7)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2024.2.2)\n","Collecting sklearn\n","  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─\u003e\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─\u003e\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy\u003e=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n","Requirement already satisfied: scipy\u003e=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n"]}],"source":["!pip install transformers\n","!pip install sklearn\n","\n","!pip install scikit-learn"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":14074,"status":"ok","timestamp":1713975356457,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"},"user_tz":-480},"id":"laV7GzBo16mr"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, TrainingArguments, Trainer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from transformers import get_linear_schedule_with_warmup, AdamW\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from sklearn.preprocessing import LabelEncoder\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21519,"status":"ok","timestamp":1713975377957,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"},"user_tz":-480},"id":"vnDY5GorRVR9","outputId":"ca628053-88c7-4ca2-fe34-e9721e9c1a03"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Encode the 'impact_duration' labels to numeric values\n","label_encoder = LabelEncoder()\n","df['impact_duration_encoded'] = label_encoder.fit_transform(df['Impact_Duration'])\n","# Check for NaN values in 'news_content'\n","if df['news_content'].isnull().any():\n","    # Handle NaN values, e.g., by replacing them with a placeholder string\n","    df['news_content'].fillna('No content', inplace=True)\n","\n","# Split the data into training and validation sets (80% training, 20% validation)\n","train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","# Initialize the tokenizer\n","tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n","\n","\n","def tokenize_data(df):\n","    # Ensure all entries are strings and handle missing values if necessary\n","    texts = df['news_content'].astype(str).tolist()  # Convert to string to avoid issues\n","    return tokenizer(texts, truncation=True, padding=True, max_length=512)\n","\n","\n","train_encodings = tokenize_data(train_df)\n","val_encodings = tokenize_data(val_df)\n","\n","train_labels = train_df['impact_duration_encoded'].tolist()\n","val_labels = val_df['impact_duration_encoded'].tolist()\n","\n","# Create a custom dataset for PyTorch\n","class NewsDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = NewsDataset(train_encodings, train_labels)\n","val_dataset = NewsDataset(val_encodings, val_labels)\n","\n","# Load the BERT model\n","model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=len(label_encoder.classes_))\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":612,"status":"ok","timestamp":1713975378552,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"},"user_tz":-480},"id":"nDfAkCoVC2Do","outputId":"5d95c445-82fd-49fc-a34a-43423e139d02"},"outputs":[{"data":{"text/plain":["XLMRobertaForSequenceClassification(\n","  (roberta): XLMRobertaModel(\n","    (embeddings): XLMRobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): XLMRobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x XLMRobertaLayer(\n","          (attention): XLMRobertaAttention(\n","            (self): XLMRobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): XLMRobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): XLMRobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): XLMRobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): XLMRobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n","  )\n",")"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=5e-5)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31515,"status":"ok","timestamp":1713975410064,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"},"user_tz":-480},"id":"-5yZF4kTSXRc","outputId":"118199c9-fc83-4541-bc4e-ab3b2e6d3ee6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers\u003c0.20,\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors\u003e=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec\u003e=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.19.3-\u003etransformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.19.3-\u003etransformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.7)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2024.2.2)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.3)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch\u003e=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n","Requirement already satisfied: safetensors\u003e=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (3.13.4)\n","Requirement already satisfied: typing-extensions\u003e=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.10.0-\u003eaccelerate) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107-\u003etorch\u003e=1.10.0-\u003eaccelerate) (12.4.127)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub-\u003eaccelerate) (2.31.0)\n","Requirement already satisfied: tqdm\u003e=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub-\u003eaccelerate) (4.66.2)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch\u003e=1.10.0-\u003eaccelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub-\u003eaccelerate) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub-\u003eaccelerate) (3.7)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub-\u003eaccelerate) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub-\u003eaccelerate) (2024.2.2)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch\u003e=1.10.0-\u003eaccelerate) (1.3.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy\u003e=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n","Requirement already satisfied: scipy\u003e=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n"]}],"source":["!pip install transformers -U\n","!pip install accelerate -U\n","#!pip install accelerate ==0.27.2\n","!pip install scikit-learn"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1713975410064,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"},"user_tz":-480},"id":"cTm0f8TKSXT7","outputId":"d2726041-7fd8-4b5a-87e9-0ba491f33c99"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.29.3\n"]}],"source":["import accelerate\n","print(accelerate.__version__)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"executionInfo":{"elapsed":490906,"status":"ok","timestamp":1713975900953,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"},"user_tz":-480},"id":"ebXCTsCvSXWa","outputId":"c41ac03a-449c-42e0-caf2-14eea90aac71"},"outputs":[{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [2250/2250 08:02, Epoch 3/3]\n","    \u003c/div\u003e\n","    \u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n"," \u003ctr style=\"text-align: left;\"\u003e\n","      \u003cth\u003eEpoch\u003c/th\u003e\n","      \u003cth\u003eTraining Loss\u003c/th\u003e\n","      \u003cth\u003eValidation Loss\u003c/th\u003e\n","      \u003cth\u003eAccuracy\u003c/th\u003e\n","      \u003cth\u003eF1 Weighted\u003c/th\u003e\n","      \u003cth\u003eF1 Macro\u003c/th\u003e\n","      \u003cth\u003eF1 Micro\u003c/th\u003e\n","      \u003cth\u003ePrecision Weighted\u003c/th\u003e\n","      \u003cth\u003eRecall Weighted\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e0.749300\u003c/td\u003e\n","      \u003ctd\u003e0.327256\u003c/td\u003e\n","      \u003ctd\u003e0.916000\u003c/td\u003e\n","      \u003ctd\u003e0.916524\u003c/td\u003e\n","      \u003ctd\u003e0.916209\u003c/td\u003e\n","      \u003ctd\u003e0.916000\u003c/td\u003e\n","      \u003ctd\u003e0.918753\u003c/td\u003e\n","      \u003ctd\u003e0.916000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003e0.204100\u003c/td\u003e\n","      \u003ctd\u003e0.193880\u003c/td\u003e\n","      \u003ctd\u003e0.962667\u003c/td\u003e\n","      \u003ctd\u003e0.962498\u003c/td\u003e\n","      \u003ctd\u003e0.962273\u003c/td\u003e\n","      \u003ctd\u003e0.962667\u003c/td\u003e\n","      \u003ctd\u003e0.963060\u003c/td\u003e\n","      \u003ctd\u003e0.962667\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3\u003c/td\u003e\n","      \u003ctd\u003e0.074600\u003c/td\u003e\n","      \u003ctd\u003e0.112920\u003c/td\u003e\n","      \u003ctd\u003e0.981333\u003c/td\u003e\n","      \u003ctd\u003e0.981344\u003c/td\u003e\n","      \u003ctd\u003e0.981179\u003c/td\u003e\n","      \u003ctd\u003e0.981333\u003c/td\u003e\n","      \u003ctd\u003e0.981371\u003c/td\u003e\n","      \u003ctd\u003e0.981333\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\u003cp\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [94/94 00:05]\n","    \u003c/div\u003e\n","    "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.1129203587770462, 'eval_accuracy': 0.9813333333333333, 'eval_f1_weighted': 0.9813442737672544, 'eval_f1_macro': 0.9811787189848458, 'eval_f1_micro': 0.9813333333333333, 'eval_precision_weighted': 0.9813709338680413, 'eval_recall_weighted': 0.9813333333333333, 'eval_runtime': 5.2273, 'eval_samples_per_second': 286.954, 'eval_steps_per_second': 17.982, 'epoch': 3.0}\n"]}],"source":["# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=16,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    evaluation_strategy=\"epoch\",\n",")\n","\n","# Compute metrics function for evaluation\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=1)\n","\n","    # Calculate weighted precision, recall, and F1\n","    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n","        labels, predictions, average='weighted')\n","\n","    # Calculate macro F1\n","    _, _, f1_macro, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n","\n","    # Calculate micro F1\n","    _, _, f1_micro, _ = precision_recall_fscore_support(labels, predictions, average='micro')\n","\n","    # Calculate accuracy\n","    acc = accuracy_score(labels, predictions)\n","\n","    # Return all metrics\n","    return {\n","        'accuracy': acc,\n","        'f1_weighted': f1_weighted,\n","        'f1_macro': f1_macro,\n","        'f1_micro': f1_micro,\n","        'precision_weighted': precision_weighted,\n","        'recall_weighted': recall_weighted\n","    }\n","\n","# Initialize the trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Train the model\n","trainer.train()\n","\n","# Evaluate the model\n","evaluation_results = trainer.evaluate()\n","\n","print(evaluation_results)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":433},"executionInfo":{"elapsed":40282,"status":"ok","timestamp":1713975941213,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"},"user_tz":-480},"id":"iG1OkffkRVUW","outputId":"71a99ece-6713-49e5-a110-65dba3c08b52"},"outputs":[{"data":{"text/html":[],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Training Dataset Metrics:\n","Accuracy: 0.9971661943657276\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00      1982\n","           1       1.00      1.00      1.00      2005\n","           2       1.00      0.99      1.00      2012\n","\n","    accuracy                           1.00      5999\n","   macro avg       1.00      1.00      1.00      5999\n","weighted avg       1.00      1.00      1.00      5999\n","\n","Validation Dataset Metrics:\n","Accuracy: 0.9813333333333333\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.99      0.99       515\n","           1       0.98      0.97      0.98       493\n","           2       0.97      0.98      0.98       492\n","\n","    accuracy                           0.98      1500\n","   macro avg       0.98      0.98      0.98      1500\n","weighted avg       0.98      0.98      0.98      1500\n","\n"]}],"source":["\n","# Generate predictions for the training dataset\n","train_predictions = trainer.predict(train_dataset)\n","\n","# Generate predictions for the validation dataset\n","val_predictions = trainer.predict(val_dataset)\n","\n","# The predictions are in logits, so you need to apply softmax to convert to probabilities and then take the argmax to get the predicted labels\n","train_pred_labels = np.argmax(train_predictions.predictions, axis=1)\n","val_pred_labels = np.argmax(val_predictions.predictions, axis=1)\n","\n","# The true labels are already provided as part of the datasets\n","train_true_labels = train_predictions.label_ids\n","val_true_labels = val_predictions.label_ids\n","\n","# Calculate accuracy for the training dataset\n","train_accuracy = accuracy_score(train_true_labels, train_pred_labels)\n","\n","# Calculate accuracy for the validation dataset\n","val_accuracy = accuracy_score(val_true_labels, val_pred_labels)\n","\n","# To print classification reports, you'll need to import it\n","from sklearn.metrics import classification_report\n","\n","# Convert the label encoder's classes to strings for use as target names\n","target_names = [str(label) for label in label_encoder.inverse_transform(range(len(label_encoder.classes_)))]\n","\n","# Then use these target names in your classification report\n","print(\"Training Dataset Metrics:\")\n","print(f\"Accuracy: {train_accuracy}\")\n","print(classification_report(train_true_labels, train_pred_labels, target_names=target_names))\n","\n","print(\"Validation Dataset Metrics:\")\n","print(f\"Accuracy: {val_accuracy}\")\n","print(classification_report(val_true_labels, val_pred_labels, target_names=target_names))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PH6C9nVOA7k4"},"source":["Adam optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":130},"id":"JtPuOBR1VL3Y"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [2250/2250 07:43, Epoch 3/3]\n","    \u003c/div\u003e\n","    \u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n"," \u003ctr style=\"text-align: left;\"\u003e\n","      \u003cth\u003eEpoch\u003c/th\u003e\n","      \u003cth\u003eTraining Loss\u003c/th\u003e\n","      \u003cth\u003eValidation Loss\u003c/th\u003e\n","      \u003cth\u003eAccuracy\u003c/th\u003e\n","      \u003cth\u003eF1 Weighted\u003c/th\u003e\n","      \u003cth\u003eF1 Macro\u003c/th\u003e\n","      \u003cth\u003eF1 Micro\u003c/th\u003e\n","      \u003cth\u003ePrecision Weighted\u003c/th\u003e\n","      \u003cth\u003eRecall Weighted\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e0.171600\u003c/td\u003e\n","      \u003ctd\u003e0.133593\u003c/td\u003e\n","      \u003ctd\u003e0.978667\u003c/td\u003e\n","      \u003ctd\u003e0.978670\u003c/td\u003e\n","      \u003ctd\u003e0.978668\u003c/td\u003e\n","      \u003ctd\u003e0.978667\u003c/td\u003e\n","      \u003ctd\u003e0.978783\u003c/td\u003e\n","      \u003ctd\u003e0.978667\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003e0.050400\u003c/td\u003e\n","      \u003ctd\u003e0.120807\u003c/td\u003e\n","      \u003ctd\u003e0.984667\u003c/td\u003e\n","      \u003ctd\u003e0.984653\u003c/td\u003e\n","      \u003ctd\u003e0.984581\u003c/td\u003e\n","      \u003ctd\u003e0.984667\u003c/td\u003e\n","      \u003ctd\u003e0.984690\u003c/td\u003e\n","      \u003ctd\u003e0.984667\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3\u003c/td\u003e\n","      \u003ctd\u003e0.010100\u003c/td\u003e\n","      \u003ctd\u003e0.110501\u003c/td\u003e\n","      \u003ctd\u003e0.987333\u003c/td\u003e\n","      \u003ctd\u003e0.987331\u003c/td\u003e\n","      \u003ctd\u003e0.987300\u003c/td\u003e\n","      \u003ctd\u003e0.987333\u003c/td\u003e\n","      \u003ctd\u003e0.987340\u003c/td\u003e\n","      \u003ctd\u003e0.987333\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\u003cp\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=2250, training_loss=0.07799337196350098, metrics={'train_runtime': 463.8877, 'train_samples_per_second': 38.796, 'train_steps_per_second': 4.85, 'total_flos': 2497105641190860.0, 'train_loss': 0.07799337196350098, 'epoch': 3.0})"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import get_linear_schedule_with_warmup, AdamW\n","\n","\n","# Example of setting up an optimizer with weight decay\n","optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n","\n","# Example of setting up a learning rate scheduler\n","num_training_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","# Update training_args to include gradient clipping\n","training_args.gradient_accumulation_steps = 1\n","training_args.max_grad_norm = 1.0\n","\n","# Implementing a Trainer with custom optimizer and scheduler\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=compute_metrics,\n","    optimizers=(optimizer, scheduler)  # Pass custom optimizer and scheduler\n",")\n","\n","# Fine-tuning with the Trainer\n","trainer.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HY6xiDWyVL54"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Dataset Metrics:\n","Accuracy: 0.9971661943657276\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00      1982\n","           1       1.00      1.00      1.00      2005\n","           2       1.00      0.99      1.00      2012\n","\n","    accuracy                           1.00      5999\n","   macro avg       1.00      1.00      1.00      5999\n","weighted avg       1.00      1.00      1.00      5999\n","\n","Validation Dataset Metrics:\n","Accuracy: 0.9813333333333333\n"]}],"source":["from sklearn.metrics import accuracy_score\n","\n","# Calculate accuracy for the training dataset\n","train_accuracy = accuracy_score(train_true_labels, train_pred_labels)\n","\n","# Calculate accuracy for the validation dataset\n","val_accuracy = accuracy_score(val_true_labels, val_pred_labels)\n","\n","# Print accuracy along with the classification report\n","print(\"Training Dataset Metrics:\")\n","print(f\"Accuracy: {train_accuracy}\")\n","print(classification_report(train_true_labels, train_pred_labels))\n","\n","print(\"Validation Dataset Metrics:\")\n","print(f\"Accuracy: {val_accuracy}\")\n","# print(classification_report(val_true_labels, val_pred_labels))"]},{"cell_type":"markdown","metadata":{"id":"aZBZNC1-FdP5"},"source":["Trainer - learning rate adjustment"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QXWktN20Fc_H"},"outputs":[{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [2250/2250 07:16, Epoch 3/3]\n","    \u003c/div\u003e\n","    \u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n"," \u003ctr style=\"text-align: left;\"\u003e\n","      \u003cth\u003eEpoch\u003c/th\u003e\n","      \u003cth\u003eTraining Loss\u003c/th\u003e\n","      \u003cth\u003eValidation Loss\u003c/th\u003e\n","      \u003cth\u003eAccuracy\u003c/th\u003e\n","      \u003cth\u003eF1 Weighted\u003c/th\u003e\n","      \u003cth\u003eF1 Macro\u003c/th\u003e\n","      \u003cth\u003eF1 Micro\u003c/th\u003e\n","      \u003cth\u003ePrecision Weighted\u003c/th\u003e\n","      \u003cth\u003eRecall Weighted\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e0.060200\u003c/td\u003e\n","      \u003ctd\u003e0.299787\u003c/td\u003e\n","      \u003ctd\u003e0.948667\u003c/td\u003e\n","      \u003ctd\u003e0.948614\u003c/td\u003e\n","      \u003ctd\u003e0.948567\u003c/td\u003e\n","      \u003ctd\u003e0.948667\u003c/td\u003e\n","      \u003ctd\u003e0.950012\u003c/td\u003e\n","      \u003ctd\u003e0.948667\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003e0.071600\u003c/td\u003e\n","      \u003ctd\u003e0.130056\u003c/td\u003e\n","      \u003ctd\u003e0.978667\u003c/td\u003e\n","      \u003ctd\u003e0.978667\u003c/td\u003e\n","      \u003ctd\u003e0.978663\u003c/td\u003e\n","      \u003ctd\u003e0.978667\u003c/td\u003e\n","      \u003ctd\u003e0.978933\u003c/td\u003e\n","      \u003ctd\u003e0.978667\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3\u003c/td\u003e\n","      \u003ctd\u003e0.020900\u003c/td\u003e\n","      \u003ctd\u003e0.119384\u003c/td\u003e\n","      \u003ctd\u003e0.983333\u003c/td\u003e\n","      \u003ctd\u003e0.983328\u003c/td\u003e\n","      \u003ctd\u003e0.983251\u003c/td\u003e\n","      \u003ctd\u003e0.983333\u003c/td\u003e\n","      \u003ctd\u003e0.983423\u003c/td\u003e\n","      \u003ctd\u003e0.983333\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\u003cp\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=2250, training_loss=0.08801228099399143, metrics={'train_runtime': 436.5195, 'train_samples_per_second': 41.228, 'train_steps_per_second': 5.154, 'total_flos': 2497105641190860.0, 'train_loss': 0.08801228099399143, 'epoch': 3.0})"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Assuming you have the `Trainer` and `TrainingArguments` set up\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Number of training steps\n","num_train_steps = len(trainer.get_train_dataloader()) * training_args.num_train_epochs\n","\n","# Setup learning rate scheduler\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                            num_warmup_steps=0,\n","                                            num_training_steps=num_train_steps)\n","\n","trainer.create_optimizer_and_scheduler(num_train_steps)\n","\n","trainer.optimizer = optimizer\n","trainer.lr_scheduler = scheduler\n","\n","# Train the model\n","trainer.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NYq9KXWjGyAn"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Dataset Metrics:\n","Accuracy: 0.9971661943657276\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00      1982\n","           1       1.00      1.00      1.00      2005\n","           2       1.00      0.99      1.00      2012\n","\n","    accuracy                           1.00      5999\n","   macro avg       1.00      1.00      1.00      5999\n","weighted avg       1.00      1.00      1.00      5999\n","\n","Validation Dataset Metrics:\n","Accuracy: 0.9813333333333333\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.99      0.99       515\n","           1       0.98      0.97      0.98       493\n","           2       0.97      0.98      0.98       492\n","\n","    accuracy                           0.98      1500\n","   macro avg       0.98      0.98      0.98      1500\n","weighted avg       0.98      0.98      0.98      1500\n","\n"]}],"source":["# Calculate accuracy for the training dataset\n","train_accuracy = accuracy_score(train_true_labels, train_pred_labels)\n","\n","# Generate and print classification report for the training dataset\n","print(\"Training Dataset Metrics:\")\n","print(f\"Accuracy: {train_accuracy}\")\n","print(classification_report(train_true_labels, train_pred_labels))\n","# Calculate accuracy for the validation dataset\n","val_accuracy = accuracy_score(val_true_labels, val_pred_labels)\n","\n","# Generate and print classification report for the validation dataset\n","print(\"Validation Dataset Metrics:\")\n","print(f\"Accuracy: {val_accuracy}\")\n","print(classification_report(val_true_labels, val_pred_labels))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zpVS87SdGyDN"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPNrrIpNMlLKSYSPIYYPQWy","gpuType":"V100","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}