{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","authorship_tag":"ABX9TyMpa7rRlNXhtp2W1FnSECOF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h0o4k90AK5jX","executionInfo":{"status":"ok","timestamp":1713972666523,"user_tz":-480,"elapsed":4581,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"}},"outputId":"0755e23e-052b-45d8-c0bd-576c681ab723"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# 連結Google drive檔案\n","from google.colab import drive\n","import pandas as pd\n","drive.mount('/content/drive', force_remount=True)\n","\n","file_path =(\"/content/EN_generated.xlsx\")\n"]},{"cell_type":"code","source":["!pip install openpyxl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IIPRSRXQLmu_","executionInfo":{"status":"ok","timestamp":1713972671575,"user_tz":-480,"elapsed":5058,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"}},"outputId":"8072ed09-a6bf-48e3-c197-1e9331be220f"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"]}]},{"cell_type":"code","source":["df = pd.read_excel(file_path)\n","\n","# Calculate the total number of news content\n","total_news_content = len(df)\n","\n","# Calculate unique news titles\n","\n","\n","# Impact Duration Distribution based on the exact values of 0, 1, and 2\n","impact_duration_distribution = df['Impact_Duration'].value_counts().sort_index()\n","\n","# Impact Level Distribution\n","\n","\n","# Output the calculated values\n","print(f\"Total News Content: {total_news_content}\")\n","\n","print(\"Impact Duration Distribution:\")\n","print(impact_duration_distribution)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mD_NM1eYRVNJ","executionInfo":{"status":"ok","timestamp":1713972671575,"user_tz":-480,"elapsed":13,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"}},"outputId":"e84b5d0d-130c-4a6a-d1e8-a25006567c5d"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Total News Content: 7499\n","Impact Duration Distribution:\n","Impact_Duration\n","0    2497\n","1    2498\n","2    2504\n","Name: count, dtype: int64\n"]}]},{"cell_type":"code","source":["!pip install transformers\n","!pip install sklearn\n","\n","!pip install scikit-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vxKvUk0DRVPf","executionInfo":{"status":"ok","timestamp":1713972696260,"user_tz":-480,"elapsed":24693,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"}},"outputId":"e2fb76e6-f7f4-4462-ba3a-9fab5fa4e049"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.26.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Collecting sklearn\n","  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertModel, BertForSequenceClassification, TrainingArguments, Trainer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from transformers import get_linear_schedule_with_warmup, AdamW\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from sklearn.preprocessing import LabelEncoder\n","import pandas as pd\n","import numpy as np"],"metadata":{"id":"OFwS2rbf1Kc7","executionInfo":{"status":"ok","timestamp":1713972696261,"user_tz":-480,"elapsed":9,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["# Encode the 'impact_duration' labels to numeric values\n","label_encoder = LabelEncoder()\n","df['impact_duration_encoded'] = label_encoder.fit_transform(df['Impact_Duration'])\n","# Check for NaN values in 'news_content'\n","if df['news_content'].isnull().any():\n","    # Handle NaN values, e.g., by replacing them with a placeholder string\n","    df['news_content'].fillna('No content', inplace=True)\n","\n","# Split the data into training and validation sets (80% training, 20% validation)\n","train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","# Initialize the tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","\n","def tokenize_data(df):\n","    # Ensure all entries are strings and handle missing values if necessary\n","    texts = df['news_content'].astype(str).tolist()  # Convert to string to avoid issues\n","    return tokenizer(texts, truncation=True, padding=True, max_length=512)\n","\n","\n","train_encodings = tokenize_data(train_df)\n","val_encodings = tokenize_data(val_df)\n","\n","train_labels = train_df['impact_duration_encoded'].tolist()\n","val_labels = val_df['impact_duration_encoded'].tolist()\n","\n","# Create a custom dataset for PyTorch\n","class NewsDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = NewsDataset(train_encodings, train_labels)\n","val_dataset = NewsDataset(val_encodings, val_labels)\n","\n","# Load the BERT model\n","model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(label_encoder.classes_))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vnDY5GorRVR9","executionInfo":{"status":"ok","timestamp":1713972718306,"user_tz":-480,"elapsed":22053,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"}},"outputId":"afd4268b-b32f-4c56-e8ba-10613e3658b5"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stderr","text":["loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/vocab.txt\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-multilingual-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.26.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 119547\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.26.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 119547\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/model.safetensors\n","Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=5e-5)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r-8Gdn29zwjE","executionInfo":{"status":"ok","timestamp":1713972719013,"user_tz":-480,"elapsed":734,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"}},"outputId":"63ec21a3-7c4b-491f-fe5d-55f0027e0819"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",")"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["!pip install transformers==4.26.0\n","!pip install accelerate==0.14.0\n","!pip install scikit-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-5yZF4kTSXRc","executionInfo":{"status":"ok","timestamp":1713972754527,"user_tz":-480,"elapsed":35516,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"}},"outputId":"b824439a-244b-4f07-bbbc-9dbe08952a97"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers==4.26.0 in /usr/local/lib/python3.10/dist-packages (4.26.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.0) (3.13.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.0) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.0) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.0) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.0) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.0) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.0) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.0) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.26.0) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.0) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.0) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.0) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.26.0) (2024.2.2)\n","Requirement already satisfied: accelerate==0.14.0 in /usr/local/lib/python3.10/dist-packages (0.14.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.14.0) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.14.0) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.14.0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.14.0) (6.0.1)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.14.0) (2.2.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.14.0) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.14.0) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.14.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.14.0) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.14.0) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.14.0) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.14.0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.14.0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.14.0) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.14.0) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.14.0) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.14.0) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.14.0) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.14.0) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.14.0) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.14.0) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.14.0) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate==0.14.0) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->accelerate==0.14.0) (12.4.127)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->accelerate==0.14.0) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->accelerate==0.14.0) (1.3.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n"]}]},{"cell_type":"code","source":["import accelerate\n","print(accelerate.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cTm0f8TKSXT7","executionInfo":{"status":"ok","timestamp":1713972754528,"user_tz":-480,"elapsed":20,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"}},"outputId":"94688fd2-ebea-4af7-8e2c-80fcf7e57dd0"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["0.14.0\n"]}]},{"cell_type":"code","source":["# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=16,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    evaluation_strategy=\"epoch\",\n",")\n","\n","# Compute metrics function for evaluation\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=1)\n","\n","    # Calculate weighted precision, recall, and F1\n","    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n","        labels, predictions, average='weighted')\n","\n","    # Calculate macro F1\n","    _, _, f1_macro, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n","\n","    # Calculate micro F1\n","    _, _, f1_micro, _ = precision_recall_fscore_support(labels, predictions, average='micro')\n","\n","    # Calculate accuracy\n","    acc = accuracy_score(labels, predictions)\n","\n","    # Return all metrics\n","    return {\n","        'accuracy': acc,\n","        'f1_weighted': f1_weighted,\n","        'f1_macro': f1_macro,\n","        'f1_micro': f1_micro,\n","        'precision_weighted': precision_weighted,\n","        'recall_weighted': recall_weighted\n","    }\n","\n","# Setup optimizer and scheduler\n","optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n","num_training_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=training_args.warmup_steps, num_training_steps=num_training_steps)\n","\n","# Initialize the trainer with custom optimizer and scheduler\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=compute_metrics,\n","    optimizers=(optimizer, scheduler)  # Pass custom optimizer and scheduler\n",")\n","\n","# Train the model\n","trainer.train()\n","\n","# Evaluate the model\n","evaluation_results = trainer.evaluate()\n","print(evaluation_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":955},"id":"ebXCTsCvSXWa","executionInfo":{"status":"ok","timestamp":1713973199041,"user_tz":-480,"elapsed":444531,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"}},"outputId":"6f077131-422e-4526-c137-8b8f572e2c84"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 5999\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2250\n","  Number of trainable parameters = 177855747\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2250/2250 07:18, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1 Weighted</th>\n","      <th>F1 Macro</th>\n","      <th>F1 Micro</th>\n","      <th>Precision Weighted</th>\n","      <th>Recall Weighted</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.631200</td>\n","      <td>0.417544</td>\n","      <td>0.905333</td>\n","      <td>0.904329</td>\n","      <td>0.904054</td>\n","      <td>0.905333</td>\n","      <td>0.911947</td>\n","      <td>0.905333</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.126300</td>\n","      <td>0.124265</td>\n","      <td>0.975333</td>\n","      <td>0.975328</td>\n","      <td>0.975232</td>\n","      <td>0.975333</td>\n","      <td>0.975725</td>\n","      <td>0.975333</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.037700</td>\n","      <td>0.091622</td>\n","      <td>0.986667</td>\n","      <td>0.986673</td>\n","      <td>0.986590</td>\n","      <td>0.986667</td>\n","      <td>0.986731</td>\n","      <td>0.986667</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 1500\n","  Batch size = 16\n","Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n","Saving model checkpoint to ./results/checkpoint-1500\n","Configuration saved in ./results/checkpoint-1500/config.json\n","Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 1500\n","  Batch size = 16\n","Saving model checkpoint to ./results/checkpoint-2000\n","Configuration saved in ./results/checkpoint-2000/config.json\n","Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 1500\n","  Batch size = 16\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Evaluation *****\n","  Num examples = 1500\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [94/94 00:04]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.0916222631931305, 'eval_accuracy': 0.9866666666666667, 'eval_f1_weighted': 0.986672741456197, 'eval_f1_macro': 0.9865898233813791, 'eval_f1_micro': 0.9866666666666668, 'eval_precision_weighted': 0.9867310168789377, 'eval_recall_weighted': 0.9866666666666667, 'eval_runtime': 4.9482, 'eval_samples_per_second': 303.143, 'eval_steps_per_second': 18.997, 'epoch': 3.0}\n"]}]},{"cell_type":"code","source":["# Step 1: Load the JSON file\n","test_file_path = '/content/EN_Test.xlsx'  # Adjust the path accordingly\n","test_df = pd.read_excel(test_file_path)\n","\n","# Calculate the total number of news content\n","total_news_content = len(test_df)\n","\n","# Calculate unique news titles\n","\n","\n","# Impact Duration Distribution based on the exact values of 0, 1, and 2\n","impact_duration_distribution = df['Impact_Duration'].value_counts().sort_index()\n","\n","# Impact Level Distribution\n","\n","\n","# Output the calculated values\n","print(f\"Total News Content: {total_news_content}\")\n","\n","print(\"Impact Duration Distribution:\")\n","print(impact_duration_distribution)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vVjCL775t9Do","executionInfo":{"status":"ok","timestamp":1713974092983,"user_tz":-480,"elapsed":379,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"}},"outputId":"34db142a-5533-469c-810a-5659faac256f"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Total News Content: 136\n","Impact Duration Distribution:\n","Impact_Duration\n","0     6\n","1    47\n","2    83\n","Name: count, dtype: int64\n"]}]},{"cell_type":"code","source":["# Convert the loaded data into a DataFrame (assuming it's a list of dictionaries)\n","test_df = pd.DataFrame(test_data)\n","\n","# Step 2: Preprocess the test data\n","label_encoder = LabelEncoder()\n","test_df['impact_duration_encoded'] = label_encoder.fit_transform(test_df['Impact_Duration'])\n","\n","if test_df['news_content'].isnull().any():\n","    # Handle NaN values, e.g., by replacing them with a placeholder string\n","    test_df['news_content'].fillna('No content', inplace=True)\n","\n","# Encode labels using the same LabelEncoder used for training\n","#test_df['impact_duration_encoded'] = label_encoder.transform(test_df['Impact_Duration'])\n","\n","# Initialize the tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","# Tokenize the test data\n","test_encodings = tokenize_data(test_df)\n","test_labels = test_df['impact_duration_encoded'].tolist()\n","\n","# Create a custom dataset for PyTorch\n","class NewsDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# Create a PyTorch dataset for the test data\n","test_dataset = NewsDataset(test_encodings, test_labels)\n","\n","\n","# Step 3: Evaluate the model on the test dataset\n","test_results = trainer.evaluate(test_dataset=test_dataset)\n","print(\"Test Results:\", test_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"9s9kWkrV0QN6","executionInfo":{"status":"error","timestamp":1713974597020,"user_tz":-480,"elapsed":11336,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"}},"outputId":"fa68c0c4-5c69-4185-a743-de553463bd8a"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stderr","text":["loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/vocab.txt\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-multilingual-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.26.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 119547\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.26.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 119547\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/model.safetensors\n","Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92m<cell line: 41>\u001b[0m:\u001b[94m41\u001b[0m                                                                            \u001b[31m│\u001b[0m\n","\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n","\u001b[1;91mTypeError: \u001b[0m\u001b[1;35mTrainer.evaluate\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m got an unexpected keyword argument \u001b[32m'test_dataset'\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 41&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">41</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Trainer.evaluate</span><span style=\"font-weight: bold\">()</span> got an unexpected keyword argument <span style=\"color: #008000; text-decoration-color: #008000\">'test_dataset'</span>\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["\n","# Generate predictions for the training dataset\n","train_predictions = trainer.predict(train_dataset)\n","\n","# Generate predictions for the validation dataset\n","val_predictions = trainer.predict(val_dataset)\n","\n","# The predictions are in logits, so you need to apply softmax to convert to probabilities and then take the argmax to get the predicted labels\n","train_pred_labels = np.argmax(train_predictions.predictions, axis=1)\n","val_pred_labels = np.argmax(val_predictions.predictions, axis=1)\n","\n","# The true labels are already provided as part of the datasets\n","train_true_labels = train_predictions.label_ids\n","val_true_labels = val_predictions.label_ids\n","\n","# Calculate accuracy for the training dataset\n","train_accuracy = accuracy_score(train_true_labels, train_pred_labels)\n","\n","# Calculate accuracy for the validation dataset\n","val_accuracy = accuracy_score(val_true_labels, val_pred_labels)\n","\n","# To print classification reports, you'll need to import it\n","from sklearn.metrics import classification_report\n","\n","# Convert the label encoder's classes to strings for use as target names\n","target_names = [str(label) for label in label_encoder.inverse_transform(range(len(label_encoder.classes_)))]\n","\n","# Then use these target names in your classification report\n","print(\"Training Dataset Metrics:\")\n","print(f\"Accuracy: {train_accuracy}\")\n","print(classification_report(train_true_labels, train_pred_labels, target_names=target_names))\n","\n","print(\"Validation Dataset Metrics:\")\n","print(f\"Accuracy: {val_accuracy}\")\n","print(classification_report(val_true_labels, val_pred_labels, target_names=target_names))\n","\n"],"metadata":{"id":"iG1OkffkRVUW","executionInfo":{"status":"aborted","timestamp":1713973199043,"user_tz":-480,"elapsed":15,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JBJRX1OisV-w","executionInfo":{"status":"aborted","timestamp":1713973199043,"user_tz":-480,"elapsed":14,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Adam optimizer"],"metadata":{"id":"KsNNNTeJ43Pm"}},{"cell_type":"code","source":["from transformers import get_linear_schedule_with_warmup, AdamW\n","\n","\n","# Example of setting up an optimizer with weight decay\n","optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n","\n","# Example of setting up a learning rate scheduler\n","num_training_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","# Update training_args to include gradient clipping\n","training_args.gradient_accumulation_steps = 1\n","training_args.max_grad_norm = 1.0\n","\n","# Implementing a Trainer with custom optimizer and scheduler\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=compute_metrics,\n","    optimizers=(optimizer, scheduler)  # Pass custom optimizer and scheduler\n",")\n","\n","# Fine-tuning with the Trainer\n","trainer.train()\n"],"metadata":{"id":"8YcmGgRQ46x8","executionInfo":{"status":"aborted","timestamp":1713973199044,"user_tz":-480,"elapsed":15,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","\n","# Calculate accuracy for the training dataset\n","train_accuracy = accuracy_score(train_true_labels, train_pred_labels)\n","\n","# Calculate accuracy for the validation dataset\n","val_accuracy = accuracy_score(val_true_labels, val_pred_labels)\n","\n","# Print accuracy along with the classification report\n","print(\"Training Dataset Metrics:\")\n","print(f\"Accuracy: {train_accuracy}\")\n","print(classification_report(train_true_labels, train_pred_labels))\n","\n","print(\"Validation Dataset Metrics:\")\n","print(f\"Accuracy: {val_accuracy}\")\n","# print(classification_report(val_true_labels, val_pred_labels))"],"metadata":{"id":"SlCJcjK54671","executionInfo":{"status":"aborted","timestamp":1713973199044,"user_tz":-480,"elapsed":15,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1wvL9VNc46-y","executionInfo":{"status":"aborted","timestamp":1713973199045,"user_tz":-480,"elapsed":15,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the test dataset\n","test_dataset_path = '/content/ML-ESG-3_Test_output_Chinese.json'\n","test_df = pd.read_json(test_dataset_path)\n","\n","# Display the first few rows of the test dataset to understand its structure\n","test_df.head()"],"metadata":{"id":"JtPuOBR1VL3Y","executionInfo":{"status":"aborted","timestamp":1713973199047,"user_tz":-480,"elapsed":17,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","from torch.utils.data import Dataset\n","import torch\n","import numpy as np\n","\n","\n","# Assuming you have already loaded your model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","\n","# Tokenize the test dataset\n","def tokenize_data(df):\n","    texts = df['news_content'].astype(str).tolist()  # Ensure all entries are strings\n","    return tokenizer(texts, truncation=True, padding=True, max_length=512)\n","\n","test_encodings = tokenize_data(test_df)\n","\n","# Dummy labels for creating the Dataset object, since labels are not needed for prediction\n","test_labels = [0] * len(test_df)\n","\n","class TestDataset(Dataset):\n","    def __init__(self, encodings):\n","        self.encodings = encodings\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        return item\n","\n","    def __len__(self):\n","        return len(self.encodings.input_ids)\n","\n","test_dataset = TestDataset(test_encodings)\n","\n","# Predict using the trained model\n","predictions = trainer.predict(test_dataset)\n","\n","# Convert predictions to labels\n","predicted_labels = np.argmax(predictions.predictions, axis=1)\n","original_labels = label_encoder.inverse_transform(predicted_labels)\n","\n","# Add predictions to the test DataFrame\n","test_df['predicted_impact_duration'] = original_labels\n","import os\n","print(os.getcwd())\n","\n","# Save the updated DataFrame to a new JSON file in the current working directory\n","output_filename = 'updated_test_dataset_with_predictions.json'\n","test_df.to_json(output_filename, orient='records', lines=True)\n","\n","print(f\"Updated test dataset saved to {output_filename}\")\n"],"metadata":{"id":"HY6xiDWyVL54","executionInfo":{"status":"aborted","timestamp":1713973199047,"user_tz":-480,"elapsed":17,"user":{"displayName":"Tina Kao","userId":"07798037423357246207"}}},"execution_count":null,"outputs":[]}]}